#version: "3.9"

x-volumes: &volumes
  # Modify the paths before the ':' in next 4 lines to match your system
    - ${HOME}/exp:/exp
    - ${HOME}/unity-envs:/unity-envs
    - ${HOME}/workspaces/navsim:/opt/navsim # only needed if you are developing navsim
    - /data:/data # only needed if your exp, unity-envs or navsim folders are symlinked from here
    - /tmp/.X11-unix:/tmp/.X11-unix:rw
    - ${XAUTHORITY}:${XAUTHORITY}
    - $XDG_RUNTIME_DIR:$XDG_RUNTIME_DIR
    - /usr/share/vulkan/icd.d:/usr/share/vulkan/icd.d

x-service-runtime: &service-runtime
    volumes: *volumes
    environment:
      - DISPLAY
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - XAUTHORITY
      - XDG_RUNTIME_DIR
    working_dir: /exp

x-deploy-nvidia: &deploy-nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              #device_ids: ['0', '3']
              capabilities: [gpu]

x-image: &img
    image: ghcr.io/ucf-sttc/navsim/navsim:1.0.0-navsim

services:
  navsim: &navsim
    <<: [*service-runtime, *deploy-nvidia, *img]
    container_name: navsim-1
    command: bash -c "while sleep 1000; do :; done"

  navsim-test:
    <<: [*navsim]
    command: bash -c "nvidia-smi; vulkaninfo; which python; python -V;
             which conda; conda -V; which mamba; mamba -V; echo 'torch.cuda:';
             python -c 'import torch; print(torch.cuda.is_available())'"

  navsim-build:
    <<: [*img]
    build: 
      context: ./tools
      dockerfile: dockerfiles/Dockerfile-navsim
      args:
        - from=debian:11.6-slim

  navsim-fixid:
    <<: [*img]
    build: 
      context: ./tools
      dockerfile: dockerfiles/Dockerfile-navsim-fixid
      args: *img

  nvidia-test:
    <<: [*deploy-nvidia, *service-runtime]
    image: debian:11.6-slim
    container_name: nvidia-test
    command: bash -c "nvidia-smi"